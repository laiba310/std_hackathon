---
title: "Module 4: Vision-Language-Action (VLA)"
---
# Module 4: Vision-Language-Action (VLA)

Welcome to Module 4: Vision-Language-Action (VLA)! In this culminating module, we bring together the power of advanced AI models with physical robotics, focusing on how large language models (LLMs) can bridge the gap between human intent and robot execution. We will explore the paradigm of VLA, where robots interpret complex natural language commands, perceive their environment through vision, and translate these into a sequence of physical actions. We will also delve into integrating voice command capabilities using tools like OpenAI Whisper. This module represents the frontier of intelligent robotics, enabling more intuitive and powerful human-robot collaboration.

## Chapters

- [Vision-Language-Action (VLA): LLMs and Robotics](01-vision-language-action.md)
- [Voice Command with OpenAI Whisper for Robot Control](02-voice-command-whisper.md)